\documentclass[a4paper,12pt,headsepline]{scrartcl} % Using KOMA-Script for better typography
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{times} % Using Times font for a classic look
\usepackage{xcolor}
\usepackage{caption}
\usepackage{listings}
\usepackage{tikz} % For advanced title page design
\usepackage{siunitx} % For formatting numbers in tables
\usepackage{array} % For better table column control

% --- Professional Color & Style Definitions ---
\definecolor{IITJBlue}{RGB}{0, 51, 102}      
\definecolor{TextGray}{RGB}{45, 45, 45}     
\definecolor{HyperlinkBlue}{RGB}{0, 80, 155} 
\definecolor{CodeBackground}{RGB}{245, 245, 245}
\definecolor{CodeGray}{RGB}{128, 128, 128}

\hypersetup{
    colorlinks=true, linkcolor=HyperlinkBlue, filecolor=HyperlinkBlue,      
    urlcolor=HyperlinkBlue, citecolor=HyperlinkBlue
}

\geometry{margin=1in}
\color{TextGray}
\setkomafont{section}{\large\bfseries\color{IITJBlue}}
\setkomafont{subsection}{\bfseries\color{IITJBlue}}

% --- Python Code Block Styling ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{CodeBackground},   
    commentstyle=\color{CodeGray}\itshape,
    keywordstyle=\color{IITJBlue},
    numberstyle=\tiny\color{CodeGray},
    stringstyle=\color{HyperlinkBlue},
    basicstyle=\footnotesize\ttfamily,
    breakatwhitespace=false, breaklines=true, captionpos=b, keepspaces=true,                 
    numbers=left, numbersep=5pt, showspaces=false, showstringspaces=false,
    showtabs=false, tabsize=2
}
\lstset{style=mystyle}

% --- Header and Footer Setup ---
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textcolor{IITJBlue}{\small Machine Learning in Economics}}
\fancyhead[R]{\textcolor{IITJBlue}{\small Anushk Gupta, AIL 7310}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

\begin{document}

% --- Professional Title Page ---
\begin{titlepage}
    \centering
    \vspace*{2cm} % Add vertical space at the top
    
    % Placeholder for course logo
    \includegraphics[width=0.5\textwidth]{cv_logo.png} % Replace with actual logo file or remove if not needed
    
    \vspace{1cm} % Add space after logo
    
    {\huge\color{IITJBlue} Machine Learning in Economics}
    
    \vspace{0.5cm}
    
    {\Large\itshape Assignment 02}
    
    \vfill % Pushes the content below to the bottom
    
    {\large\bfseries Anushk Gupta} \\
    {\mdseries AIL 7310}
    
    \vspace{0.8cm}
    
    {\large \href{https://colab.research.google.com/drive/1gCpk-J7ZFCZzIAV6karfv8xuHTEOVyvK?usp=sharing}{\color{HyperlinkBlue}Link to Google Colab Notebook}}
    
    \vspace{1.5cm}
    
    {\large \today}
    
\end{titlepage}

\tableofcontents
\newpage

\section{Abstract}
This report addresses all eight questions of Assignment 2 by applying machine learning methods to predict credit card defaults using the \texttt{credit\_risk.csv} dataset. We compare Decision Trees (DT), Random Forests (RF), Gradient Boosted Trees (GBT), Conditional Inference Trees (ctree), and Conditional Inference Forests (CIF). Model performance is evaluated using accuracy, precision, recall, F1-score, and ROC AUC on a 20\% held-out test set. The analysis highlights the superior performance of Conditional Inference Forests, with a ROC AUC of 0.93, and provides insights into feature importance and model differences.

\section{Introduction}
Credit risk modeling is essential in economics and finance for managing lending portfolios. This report explores tree-based machine learning models to predict credit card defaults, addressing dataset characteristics, model performance, and feature importance as required by the assignment questions.

\section{Data Description (Question 1)}
The \texttt{credit\_risk.csv} dataset contains 5,000 observations of individuals with socio-economic variables: \texttt{income}, \texttt{age}, \texttt{hh\_size}, \texttt{employed}, \texttt{assets}, \texttt{loan\_amount}, \texttt{credit\_score}, \texttt{subsidy}, \texttt{urban}, and \texttt{region}. The target variable, \texttt{default}, indicates whether an individual defaulted (1) or not (0).

\subsection{Summary Statistics}
The summary statistics for the dataset variables are presented below:

\begin{table}[h]
\centering
\small
\resizebox{\textwidth}{!}{
\begin{tabular}{l *{10}{S[table-format=6.2,group-digits=true,group-separator={,}]}}
\toprule
Statistic & {income} & {age} & {hh\_size} & {employed} & {assets} & {loan\_amount} & {credit\_score} & {subsidy} & {urban} & {default} \\
\midrule
count & 5000 & 5000 & 5000 & 5000 & 5000 & 5000 & 5000 & 5000 & 5000 & 5000 \\
mean & 59657 & 45.11 & 3.03 & 0.85 & 89832 & 27660 & 538.94 & 0.20 & 0.59 & 0.28 \\
std & 20013 & 14.04 & 1.41 & 0.36 & 47394 & 12905 & 145.57 & 0.40 & 0.49 & 0.45 \\
min & 5000 & 21 & 1 & 0 & 3562 & 5000 & 300 & 0 & 0 & 0 \\
25\% & 46226 & 33 & 2 & 1 & 52003 & 16487 & 421 & 0 & 0 & 0 \\
50\% & 59435 & 45.5 & 3 & 1 & 82016 & 27692 & 537 & 0 & 1 & 0 \\
75\% & 72883 & 57 & 4 & 1 & 119953 & 38852 & 652 & 0 & 1 & 1 \\
max & 133389 & 69 & 5 & 1 & 316311 & 49996 & 850 & 1 & 1 & 1 \\
\bottomrule
\end{tabular}
}
\caption{Summary statistics of the \texttt{credit\_risk.csv} dataset. Note: \texttt{region} is categorical and excluded from numerical statistics.}
\label{tab:summary_stats}
\end{table}

\subsection{Data Characteristics}
\begin{itemize}
    \item \textbf{Missing Values}: No missing values were found in the dataset.
    \item \textbf{Data Types}: Most variables (\texttt{income}, \texttt{age}, \texttt{hh\_size}, \texttt{assets}, \texttt{loan\_amount}, \texttt{credit\_score}) are numeric (float or integer). \texttt{employed}, \texttt{subsidy}, \texttt{urban}, and \texttt{default} are binary (0 or 1). \texttt{region} is categorical.
    \item \textbf{Class Balance}: The target variable \texttt{default} is imbalanced, with 28.4\% defaults (1) and 71.6\% non-defaults (0).
    \item \textbf{Train/Test Split}: The dataset was split into 4,000 training observations (80\%) and 1,000 test observations (20\%), each with 10 features.
\end{itemize}

\section{Methodology}
Four tree-based models were implemented to predict credit card defaults, with hyperparameter tuning performed via 5-fold cross-validation. A 20\% held-out test set was used for evaluation. The models are:
\begin{enumerate}
    \item \textbf{Decision Tree (DT)}: Tuned parameters include \texttt{max\_depth} and \texttt{min\_samples\_leaf}.
    \item \textbf{Random Forest (RF)}: Tuned parameters include \texttt{n\_estimators} and \texttt{max\_features}.
    \item \textbf{Gradient Boosted Trees (GBT)}: Tuned parameters include \texttt{n\_estimators}, \texttt{learning\_rate}, and \texttt{max\_depth}.
    \item \textbf{Conditional Inference Tree (ctree) and Forest (CIF)}: Implemented using the R package \texttt{party}.
\end{enumerate}
The complete workflow is available in the Google Colab notebook linked on the title page.

\section{Results}

\subsection{Decision Tree (Question 2)}
The Decision Tree model was tuned with \texttt{max\_depth=3} and \texttt{min\_samples\_leaf=10}. Performance on the test set is as follows:
\begin{itemize}
    \item \textbf{Metrics}: Accuracy 0.727, Precision 0.59, Recall 0.13, F1-score 0.21, ROC AUC 0.75.
    \item \textbf{Classification Report}:
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Class & Precision & Recall & F1-score \\
\midrule
No Default (0) & 0.74 & 0.96 & 0.83 \\
Default (1) & 0.59 & 0.13 & 0.21 \\
\bottomrule
\end{tabular}
\caption{Classification report for Decision Tree.}
\label{tab:dt_classification}
\end{table}

\subsection{Random Forest (Question 3)}
The Random Forest model was tuned with \texttt{n\_estimators=200} and \texttt{max\_features=sqrt}. Performance on the test set is:
\begin{itemize}
    \item \textbf{Metrics}: Accuracy 0.717, Precision 0.50, Recall 0.30, F1-score 0.38, ROC AUC 0.75.
    \item \textbf{Classification Report}:
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Class & Precision & Recall & F1-score \\
\midrule
No Default (0) & 0.76 & 0.88 & 0.82 \\
Default (1) & 0.50 & 0.30 & 0.38 \\
\bottomrule
\end{tabular}
\caption{Classification report for Random Forest.}
\label{tab:rf_classification}
\end{table}

\subsection{Gradient Boosted Trees (Question 4)}
The Gradient Boosted Trees model was tuned with \texttt{learning\_rate=0.02}, \texttt{max\_depth=2}, and \texttt{n\_estimators=100}. Performance on the test set is:
\begin{itemize}
    \item \textbf{Metrics}: Accuracy 0.726, Precision 0.57, Recall 0.14, F1-score 0.23, ROC AUC 0.75.
    \item \textbf{Classification Report}:
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Class & Precision & Recall & F1-score \\
\midrule
No Default (0) & 0.74 & 0.96 & 0.83 \\
Default (1) & 0.57 & 0.14 & 0.23 \\
\bottomrule
\end{tabular}
\caption{Classification report for Gradient Boosted Trees.}
\label{tab:gbt_classification}
\end{table}

\subsection{Comparison of Metrics (Question 5)}
The performance metrics for all models are summarized below:

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\toprule
Model & Accuracy & Precision & Recall & F1-score & ROC AUC \\
\midrule
Decision Tree & 0.727 & 0.59 & 0.13 & 0.21 & 0.75 \\
Random Forest & 0.717 & 0.50 & 0.30 & 0.38 & 0.75 \\
Gradient Boosted Trees & 0.726 & 0.57 & 0.14 & 0.23 & 0.75 \\
Conditional Inference Forest & 0.852 & 0.87 & 0.56 & 0.68 & 0.93 \\
\bottomrule
\end{tabular}
\caption{Performance metrics for all models on test data.}
\label{tab:performance_comparison}
\end{table}

\subsection{Conditional Inference Trees and Forests (Question 7)}
Both Conditional Inference Tree (ctree) and Conditional Inference Forest (CIF) were implemented using the R package \texttt{party}. The single ctree model showed weaker predictive power compared to CIF and is not reported in detail. The CIF model achieved the best performance:
\begin{itemize}
    \item \textbf{Metrics}: Accuracy 0.852, Precision 0.87, Recall 0.56, F1-score 0.68, ROC AUC 0.93.
    \item \textbf{Feature Importance}:
\end{itemize}

\begin{table}[h]
\centering
\begin{tabular}{lr}
\toprule
Feature & Importance \\
\midrule
Credit Score & 0.44 \\
Income & 0.12 \\
Loan Amount & 0.12 \\
Assets & 0.03 \\
Subsidy & 0.02 \\
Age & 0.02 \\
\bottomrule
\end{tabular}
\caption{Feature importance for Conditional Inference Forest.}
\label{tab:cif_importance}
\end{table}

\subsection{Feature Importance Comparison (Question 8)}
Feature importance rankings differ across models:

\begin{table}[h]
\centering
\small
\resizebox{\textwidth}{!}{
\begin{tabular}{m{3cm} m{5cm} m{5cm}}
\toprule
Model & Top Features & Notes \\
\midrule
Random Forest & Income, Credit Score, Assets & Importance distributed across features \\
Gradient Boosted Trees & Income, Credit Score & Income dominates \\
Conditional Inference Forest & Credit Score, Income, Loan Amount & Clear dominance of top three \\
\bottomrule
\end{tabular}
}
\caption{Feature importance comparison across models.}
\label{tab:feature_importance}
\end{table}

The differences stem from:
\begin{enumerate}
    \item \textbf{Splitting Bias}: RF and GBT favor continuous variables with many split points, while CIF uses unbiased statistical tests.
    \item \textbf{Correlated Predictors}: CIF handles correlated features more effectively.
    \item \textbf{Sequential Learning}: GBT’s boosting process shifts importance due to sequential learning.
\end{enumerate}

\newpage

\subsection{Visualizations (Question 6)}
The following figures provide visual insights into model performance:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{ROC_curve.png}
    \caption{ROC curves for Decision Tree, Random Forest, and Gradient Boosted Trees.}
    \label{fig:roc_curves}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{confusion_matrix.png}
    \caption{Confusion matrices for Decision Tree, Random Forest, and Gradient Boosted Trees.}
    \label{fig:confusion_matrix}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{perm_imp_rf.png}
    \caption{Permutation feature importance — Random Forest.}
    \label{fig:perm_imp_rf}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{perm_imp_gbt.png}
    \caption{Permutation feature importance — Gradient Boosted Trees.}
    \label{fig:perm_imp_gbt}
\end{figure}



\section{Verdict}
Conditional Inference Forests outperformed other models due to their use of unbiased statistical tests for splitting, which reduces bias toward variables with many split points and better handles correlated predictors. The single Conditional Inference Tree (ctree) was tested but showed weaker predictive power, reinforcing the ensemble approach’s strength. Feature importance analysis highlights \texttt{credit\_score}, \texttt{income}, and \texttt{loan\_amount} as key predictors, aligning with economic intuition.

\end{document}